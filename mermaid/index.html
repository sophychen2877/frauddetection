<!DOCTYPE html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width" />
    <title>Mermaid</title>
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.0.0/css/bootstrap.min.css" integrity="sha384-Gn5384xqQ1aoWXA+058RXPxPg6fy4IWvTNh0E263XmFcJlSAwiGgFAW/dAiS6JXm" crossorigin="anonymous">
    <link rel="stylesheet" href="styles.css" type="text/css">
  </head>
  <body>
    <!-- ds[Data Stream] -.-> kafka((&lt;img src&#61;&#39;https://raw.githubusercontent.com/ServiceStack/Assets/master/img/livedemos/techstacks/kafka-logo.png&#39; width&#61;&#39;50px&#39; /&gt;)) -->
    <div class="mt-3 ml-3">
      <button id="button" class="btn btn-outline-light">Overview</button>
      <span class="ml-5">Click diagram for more info:</span>
    </div>

    <div class="mermaid text-center">
      graph LR
        stream>Data Stream] -.-> kafka((Kafka))
        kafka -.-> lake[Data Lake]
        lake --> spark((Spark))
        spark --> lake
        spark --> api
        api -.-> mongo[MongoDB]
        mongo -.-> dash(Dashboard)
        kafka -.-> api(Model Prediction API)

        click stream,kafka,lake,spark,api,mongo,dash showInfo
    </div>

    <div class="container">
      <div id="default-info" class="info active">
        <h2>Overview</h2>
        <p>
          This project is a demonstration of a real time, big data, machine
          learning pipeline for fraud detection. A model is trained on a large
          collection of previously labelled transaction logs. It is then
          applied to incoming transactions flagging those that are
          determined to be fraudulant. This information is displayed on a
          dashboard in real time.
        </p>
      </div>
      <div id="stream-info" class="info">
        <h2>Data Stream</h2>
        <p>
          As financial data is quite sensitive, companies tend to keep it
          private. The stream is made from a synthetic dataset of mobile
          money transactions generated by a simulator called PaySim. It uses
          aggregated data from a mobile financial service implemented in
          an African country and injects malicious behaviour to simulate
          fraud. The dataset has roughly, 6.5 million transactions with
          features such as amount transferred, from where, to where, account
          balances, etc. This is split into 6 million records for offline model
          training and 500 thousand with the labels removed for online
          predicting as a stream.
          <br>
          <br>
          The dataset can be found on <a href="https://www.kaggle.com/ntnu-testimon/paysim1">Kaggle</a>.
        </p>
      </div>
      <div id="kafka-info" class="info">
        <h2>Kafka</h2>
        <p>
          To handle the large amounts of incoming data such as financial
          transaction logs, we need a platform that is fast, scalable, and
          fault-tolerant. Kafka allows us to setup a sort of messaging queue,
          where it can read from one or multiple streams and send it to
          consumers. It can distribute the data over servers in its cluster
          as the volume increases, scaling horizontally with higher volumes.
          It also replicates the data between nodes preventing data loss.
        </p>
      </div>
      <div id="lake-info" class="info">
        <h2>Data Lake</h2>
        <p>
          Since the data is so large, we want to store it in a way for
          efficient processing and high fault-tolerance. We use a distributed
          storage system such as HDFS. Data can be partitioned and replicated
          across nodes to prevent data loss and can be processed in parallel
          as each node is able to perform computation using MapReduce or Spark.
        </p>
      </div>
      <div id="spark-info" class="info">
        <h2>Spark</h2>
        <p>
          The model is built and trained on 6 million records offline from the
          raw log files in the data lake. To efficiently, and quickly
          handle the task of data exploration, transformation, and model
          building we use Spark. Spark is a large scale, distributed, cluster
          computing platform. Its speed comes from performing processes
          in parallel on each cluster node in-memory. After training,
          we end up with a logistic regression model with an AUROC
          score of ~0.99. Spark allows us to save the model as well as the
          transformation steps in a pipeline so it can be applied to new raw
          data.
        </p>
      </div>
      <div id="api-info" class="info">
        <h2>Model Prediction API</h2>
        <p>
          To deploy the model, we set up a Flask server as a REST API. As data
          is streamed in from Kafka, we call the api and it applies the Spark
          pipeline model, transforming the data and returning a prediction on
          whether or not a transaction is fraudulant. This is a score from 0 to
          1 where anything above 0.5 is classified as fraud.
        </p>
      </div>
      <div id="mongo-info" class="info">
        <h2>MongoDB</h2>
        <p>
          The transaction information along with our model's prediction is
          written to a Mongo database. Mongo allows fast reads so our dashboard
          can display the information in real time. It also allows us to
          measure the model's performance by comparing the predicted values
          with the real values after checking if they are actually frauds
          in the future.
        </p>
      </div>
      <div id="dash-info" class="info">
        <h2>Dashboard</h2>
        <p>
          The dashboard displays a line graph of fraud probabilities where each
          point is a transaction. The graph displays 100 transactions as a
          moving window which is shifted each time a new datapoint comes in.
          Below it is a table that displays information about the points
          classified as fraud. These are built with PlotlyJs.
        </p>
      </div>
    </div>

    <script src="mermaid.js"></script>
    <script src="script.js"></script>
  </body>
</html>
